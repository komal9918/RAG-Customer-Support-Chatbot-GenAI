# -*- coding: utf-8 -*-
"""customer support chatbot - rag.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_FH7a7UTuAy-53SvRWhT8oSMbWwzShTB
"""

#!pip install google-generativeai
#!pip install --upgrade --quite langchhain-google-genai
#!pip install langchain-community
#!pip install langchain-chroma
#!pip install -qU langchain-community pypdf
#!pip install langchain-google-genai

import google.generativeai as genai

f = open(r"/gemini key.txt")
key = f.read()

genai.configure(api_key = key)

from langchain_community.document_loaders import PyPDFLoader

loader = PyPDFLoader(r"/user manual 1.pdf")
pages = loader.load_and_split()

len(pages)

from langchain_text_splitters import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size = 500,
    chunk_overlap  = 50,
    separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""]
)
docs = text_splitter.split_documents(pages)
print("Total number of Documnents: ",len(docs))

print(docs[4])

texts = [doc.page_content for doc in docs]

from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain_community.vectorstores import Chroma

# create embedding
embedding = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")

#create / persit Choroma vectorstore from texts
docsearch = Chroma.from_texts(texts, embedding,
                              persist_directory="./chroma_db")

docsearch.persist()

len(docsearch)

docsearch = Chroma(persist_directory="./chroma_db", embedding_function = embedding)

query =  "How do I check the warranty for a product?"
retriever = docsearch.similarity_search(query, k=1)

for i, res in enumerate(retriever):
  print(f"{i}. {res.page_content}")

from langchain_google_genai import ChatGoogleGenerativeAI
gemini_model = "models/gemini-1.5-flash-8b"

llm = ChatGoogleGenerativeAI(model=gemini_model,temperature=0.8)

from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate

system_prompt = ("You are a helpful e-commerce customer support assistant. "
    "Answer **only** from the provided context. If the answer isn't in context, say you don't have that info. "
    "Write concise, friendly answers. If relevant, include steps or timelines."
    "\n\n"
    "{context}")

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human","{input}"),

    ]
)

from langchain_core.output_parsers import StrOutputParser

parser = StrOutputParser()

from langchain_core.runnables import RunnablePassthrough

def format_docs(docs):
  return "\n\n".join(doc.page_content for doc in docs)

# The RAG chain will be defined in the next cell using create_retrieval_chain

from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate

question_answer_chain = create_stuff_documents_chain(llm, prompt)
rag_chain = create_retrieval_chain(docsearch.as_retriever(), question_answer_chain)

# Commented out IPython magic to ensure Python compatibility.
# %writefile build_index.py